# å¼€æºé¡¹ç›® Chinese-LLaMA-AIpaca



## ç¬¬ä¸€éƒ¨åˆ†ï¼šè®ºæ–‡è®²è§£

### 1. è®ºæ–‡éƒ¨åˆ†å†…å®¹è§£è¯»ï¼›





## ç¬¬äºŒéƒ¨åˆ†ï¼šæ¡ˆä¾‹å®æˆ˜



### 1. é¡¹ç›®æ¦‚è§ˆï¼ˆæ–‡æ¡£ + ä»£ç æ–‡ä»¶ï¼‰



### 2. æ¡ˆä¾‹å®æˆ˜ï¼ˆæµ‹è¯•ç”¨ä¾‹ + ç¯å¢ƒé…ç½®ï¼‰



### ç¬¬1æ­¥ï¼šç”ŸæˆæŒ‡ä»¤æ•°æ®

#### è„šæœ¬-1 : crawl_prompt.py

è¿è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š

```shell
python scripts/crawl_prompt.py test/output.txt
```



### ç¬¬2æ­¥ï¼šå°†åŸç‰ˆï¼ˆå®˜æ–¹ï¼‰LLaMAæ¨¡å‹è½¬æ¢ä¸ºHFæ ¼å¼

<font color=blue>**LLaMAåŸç‰ˆæ¨¡å‹ä¸‹è½½**Â </font>

```shell
é“¾æ¥: https://pan.baidu.com/s/17_y9BTpFys5SAhphL_EBEw 
æå–ç : nv8b
```

ä½¿ç”¨huggingface transformersæä¾›çš„è„šæœ¬ `convert_llama_weights_to_hf.py`ï¼Œå°†åŸç‰ˆLLaMAæ¨¡å‹è½¬æ¢ä¸ºHuggingFaceæ ¼å¼ã€‚å°†åŸç‰ˆLLaMAçš„ `tokenizer.model` æ”¾åœ¨ `--input_dir` æŒ‡å®šçš„ç›®å½•ï¼Œå…¶ä½™æ–‡ä»¶æ”¾åœ¨`${input_dir}/${model_size}`ä¸‹ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åï¼Œ`--output_dir`ä¸­å°†å­˜æ”¾è½¬æ¢å¥½çš„HFç‰ˆæƒé‡ã€‚

- å¯ä»¥é€šè¿‡ `git clone https://github.com/huggingface/transformers.git` ä¸‹è½½transformers

å…·ä½“çš„å‘½ä»¤å¦‚ä¸‹ï¼ˆ**<font color=red>æ³¨æ„ï¼šè·¯å¾„ç”¨ä½ è‡ªå·±çš„è·¯å¾„ï¼Œä¸è¦ç”¨æˆ‘çš„è·¯å¾„</font>**ï¼‰

â‘  è¿›å…¥åˆ°transformersç›¸åº”çš„è·¯å¾„ä¸‹ï¼Œå¦‚ä¸‹:

```shell
cd /Users/tgl/Downloads/transformers/src/transformers/models/llama
```

â‘¡ ç„¶åæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤

```shell
python convert_llama_weights_to_hf.py \
--input_dir /Users/tgl/Downloads/llama_models/ \
--model_size 7B \
--output_dir /Users/tgl/Downloads/llama_models/7B_hf/
```

å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œåˆ™ä¼šè¾“å‡ºä¸‹é¢çš„ç»“æœ:

**png-01**

![](./images/01.png)



### ç¬¬3æ­¥ï¼šåˆå¹¶LoRAæƒé‡ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡

#### æ–¹å¼1ï¼šå•LoRAæƒé‡åˆå¹¶ï¼ˆé€‚ç”¨äº Chinese-LLaMA, Chinese-LLaMA-Plus, Chinese-Alpacaï¼‰

[é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%8D%95lora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8E-chinese-llama-chinese-llama-plus-chinese-alpaca)

#### æ–¹å¼2ï¼šå¤šLoRAæƒé‡åˆå¹¶ï¼ˆé€‚ç”¨äºChinese-Alpaca-Plus ï¼‰

è¿›å…¥åˆ°é¡¹ç›®è·¯å¾„ä¸‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ï¼š

```shell
python scripts/merge_llama_with_chinese_lora.py \
--base_model /Users/tgl/Downloads/llama_models/7B_hf/ \
--lora_model /Users/tgl/Downloads/llama_models/chinese_llama_plus_lora_7b,/Users/tgl/Downloads/llama_models/chinese_alpaca_plus_lora_7b \
--output_dir /Users/tgl/Downloads/llama_models/7B_full_model
```

å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œåˆ™ä¼šè¾“å‡ºä¸‹é¢çš„ç»“æœ:

**png-02**

![](./images/02.png)



### ç¬¬4æ­¥ï¼šä½¿ç”¨ Transformers è¿›è¡Œæ¨ç†

åœ¨ä¸å®‰è£…å…¶ä»–åº“æˆ–PythonåŒ…çš„æƒ…å†µä¸‹å¿«é€Ÿä½“éªŒæ¨¡å‹æ•ˆæœï¼Œå¯ä»¥ä½¿ç”¨ [scripts/inference_hf.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/scripts/inference_hf.py) è„šæœ¬å¯åŠ¨éé‡åŒ–æ¨¡å‹ã€‚è¯¥è„šæœ¬æ”¯æŒCPUå’ŒGPUçš„å•å¡æ¨ç†ã€‚ä»¥å¯åŠ¨Chinese-Alpaca-7Bæ¨¡å‹ä¸ºä¾‹ï¼Œè„šæœ¬è¿è¡Œæ–¹å¼ï¼Œå‚è€ƒä»£ç å¦‚ä¸‹ï¼š

```python
CUDA_VISIBLE_DEVICES={device_id} python scripts/inference_hf.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_or_alpaca_lora \
    --with_prompt \
    --interactive
```

å¦‚æœå·²ç»æ‰§è¡Œäº†`merge_llama_with_chinese_lora_to_hf.py`è„šæœ¬å°†loraæƒé‡åˆå¹¶ï¼Œé‚£ä¹ˆæ— éœ€å†æŒ‡å®š`--lora_model`ï¼Œå¯åŠ¨æ–¹å¼æ›´ç®€å•ï¼Œå®é™…æ‰§è¡Œä»£ç å¦‚ä¸‹ï¼š

```shell
python scripts/inference_hf.py \
--base_model /Users/tgl/Downloads/llama_models/7B_full_model \
--with_prompt \
--interactive
```

å‚æ•°è¯´æ˜ï¼š

- `{device_id}`ï¼šCUDAè®¾å¤‡ç¼–å·ã€‚å¦‚æœä¸ºç©ºï¼Œé‚£ä¹ˆåœ¨CPUä¸Šè¿›è¡Œæ¨ç†
- `--base_model {base_model} `ï¼šå­˜æ”¾**HFæ ¼å¼**çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ã€‚å¦‚æœä¹‹å‰åˆå¹¶ç”Ÿæˆçš„æ˜¯PyTorchæ ¼å¼æ¨¡å‹ï¼Œ[è¯·è½¬æ¢ä¸ºHFæ ¼å¼](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨Transformersæ¨ç†#step-1-å°†åŸç‰ˆllamaæ¨¡å‹è½¬æ¢ä¸ºhfæ ¼å¼)
- `--lora_model {lora_model}` ï¼šä¸­æ–‡LLaMA/Alpaca LoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨[ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨Transformersæ¨ç†#Model-Hub)ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™åªåŠ è½½`--base_model`æŒ‡å®šçš„æ¨¡å‹
- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸`--lora_model`ç›¸åŒï¼›è‹¥ä¹Ÿæœªæä¾›`--lora_model`å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸`--base_model`ç›¸åŒ
- `--with_prompt`ï¼šæ˜¯å¦å°†è¾“å…¥ä¸promptæ¨¡ç‰ˆè¿›è¡Œåˆå¹¶ã€‚**å¦‚æœåŠ è½½Alpacaæ¨¡å‹ï¼Œè¯·åŠ¡å¿…å¯ç”¨æ­¤é€‰é¡¹ï¼**
- `--interactive`ï¼šä»¥äº¤äº’æ–¹å¼å¯åŠ¨ï¼Œä»¥ä¾¿è¿›è¡Œå¤šæ¬¡**å•è½®é—®ç­”**ï¼ˆæ­¤å¤„ä¸æ˜¯llama.cppä¸­çš„ä¸Šä¸‹æ–‡å¯¹è¯ï¼‰
- `--data_file {file_name}`ï¼šéäº¤äº’æ–¹å¼å¯åŠ¨ä¸‹ï¼ŒæŒ‰è¡Œè¯»å–`file_name`ä¸­çš„çš„å†…å®¹è¿›è¡Œé¢„æµ‹
- `--predictions_file {file_name}`ï¼šéäº¤äº’å¼æ–¹å¼ä¸‹ï¼Œå°†é¢„æµ‹çš„ç»“æœä»¥jsonæ ¼å¼å†™å…¥`file_name`

æ³¨æ„äº‹é¡¹ï¼š

- å› ä¸åŒæ¡†æ¶çš„è§£ç å®ç°ç»†èŠ‚æœ‰å·®å¼‚ï¼Œè¯¥è„šæœ¬å¹¶ä¸èƒ½ä¿è¯å¤ç°llama.cppçš„è§£ç æ•ˆæœ
- è¯¥è„šæœ¬ä»…ä¸ºæ–¹ä¾¿å¿«é€Ÿä½“éªŒç”¨ï¼Œå¹¶æœªå¯¹å¤šæœºå¤šå¡ã€ä½å†…å­˜ã€ä½æ˜¾å­˜ç­‰æƒ…å†µç­‰æ¡ä»¶åšä»»ä½•ä¼˜åŒ–
- å¦‚åœ¨CPUä¸Šè¿è¡Œ7Bæ¨¡å‹æ¨ç†ï¼Œè¯·ç¡®ä¿æœ‰32GBå†…å­˜ï¼›å¦‚åœ¨GPUä¸Šè¿è¡Œ7Bæ¨¡å‹æ¨ç†ï¼Œè¯·ç¡®ä¿æœ‰20GBæ˜¾å­˜



### ç¬¬5æ­¥ï¼šä½¿ç”¨ webui æ­å»ºç•Œé¢

æ¥ä¸‹æ¥ä»¥[text-generation-webuiå·¥å…·](https://github.com/oobabooga/text-generation-webui)ä¸ºä¾‹ï¼Œä»‹ç»æ— éœ€åˆå¹¶æ¨¡å‹å³å¯è¿›è¡Œ**æœ¬åœ°åŒ–éƒ¨ç½²**çš„è¯¦ç»†æ­¥:

##### Step 1: å…‹éš†text-generation-webuiå¹¶å®‰è£…å¿…è¦çš„ä¾èµ–

```shell
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
pip install -r requirements.txt
```



##### Step 2: å°†ä¸‹è½½åçš„loraæƒé‡æ”¾åˆ°lorasæ–‡ä»¶å¤¹ä¸‹

```shell
ls models/llama-7b-hf

pytorch_model-00001-of-00002.bin pytorch_model-00002-of-00002.bin config.json pytorch_model.bin.index.json generation_config.json
```



##### Step 3: å°†HuggingFaceæ ¼å¼çš„llama-7Bæ¨¡å‹æ–‡ä»¶æ”¾åˆ°modelsæ–‡ä»¶å¤¹ä¸‹



##### Step 4: å¤åˆ¶loraæƒé‡çš„tokenizeråˆ°models/llama-7b-hfä¸‹(webuié»˜è®¤ä»`./models`ä¸‹åŠ è½½tokenizer.model,å› æ­¤éœ€ä½¿ç”¨æ‰©å±•ä¸­æ–‡è¯è¡¨åçš„tokenizer.model)

```shell
cp loras/chinese-alpaca-lora-7b/tokenizer.model models/llama-7b-hf/
cp loras/chinese-alpaca-lora-7b/special_tokens_map.json models/llama-7b-hf/
cp loras/chinese-alpaca-lora-7b/tokenizer_config.json models/llama-7b-hf/
```



##### Step 5: ä¿®æ”¹/modules/LoRA.pyæ–‡ä»¶ï¼Œåœ¨`PeftModel.from_pretrained`æ–¹æ³•ä¹‹å‰æ·»åŠ ä¸€è¡Œä»£ç ä¿®æ”¹åŸå§‹llamaçš„embed_size

```shell
shared.model.resize_token_embeddings(len(shared.tokenizer))
shared.model = PeftModel.from_pretrained(shared.model, Path(f"{shared.args.lora_dir}/{lora_name}"), **params)
```



##### Step 6: æ¥ä¸‹æ¥å°±å¯ä»¥æ„‰å¿«çš„è¿è¡Œäº†ï¼Œå‚è€ƒ[webui using LoRAs](https://github.com/oobabooga/text-generation-webui/wiki/Using-LoRAs)ï¼Œæ‚¨ä¹Ÿå¯ä»¥ç›´æ¥è¿è¡Œåˆå¹¶åçš„chinese-alpaca-7bï¼Œç›¸å¯¹åŠ è½½ä¸¤ä¸ªæƒé‡æ¨ç†é€Ÿåº¦ä¼šæœ‰è¾ƒå¤§çš„æå‡



### ç¬¬6æ­¥ï¼šåŸºäºLLaMAæ¨¡å‹å’ŒLoRAæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒ

çœç•¥ï¼Œç›´æ¥çœ‹Bç«™åˆ†äº«çš„è§†é¢‘ã€‚





### ç¬¬7æ­¥ï¼šæ¡ˆä¾‹å®æˆ˜ï¼šæŒ‡ä»¤ç²¾è°ƒè„šæœ¬

#### 7.1 å•å¡è®­ç»ƒ

##### 7.1.1 æƒ…æ™¯1ï¼šç»§ç»­è®­ç»ƒ Chinese-AIpacaæ¨¡å‹çš„LoRAæƒé‡

- `--model_name_or_path`: åŸç‰ˆHFæ ¼å¼LLaMAæ¨¡å‹ï¼ˆå¦‚æœç»§ç»­è®­ç»ƒéPlus Alpacaæ¨¡å‹ï¼‰**æˆ–**åˆå¹¶Chinese-LLaMA-Plus-LoRAåçš„Chinese-LLaMAæ¨¡å‹ï¼ˆå¦‚æœç»§ç»­è®­ç»ƒPlusæ¨¡å‹ï¼‰

- `--peft_path`: Chinese-Alpacaçš„LoRAæƒé‡ç›®å½•

- æ— éœ€æŒ‡å®š`--lora_rank`ã€`--lora_alpha`ã€`--lora_dropout`ã€`--trainable`å’Œ`--modules_to_save`å‚æ•°

  

â‘  åˆå¹¶Chinese-LLaMA-Plus-LoRAåçš„Chinese-LLaMAæ¨¡å‹ï¼ˆå¦‚æœç»§ç»­è®­ç»ƒPlusæ¨¡å‹ï¼‰

`run_sft.shçš„å†…å®¹å¦‚ä¸‹`

```shell
########å‚æ•°éƒ¨åˆ†########
lr=1e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05
pretrained_model=/root/autodl-tmp/7B_full_model
chinese_tokenizer_path=/root/autodl-tmp/projects/scripts/merged_tokenizer_hf
dataset_dir=/root/autodl-tmp/projects/data/train
per_device_train_batch_size=1
per_device_eval_batch_size=1
training_steps=10
gradient_accumulation_steps=1
output_dir=/root/autodl-tmp/sft_output
peft_model=/root/autodl-tmp/chinese_alpaca_plus_lora_7b
validation_file=/root/autodl-tmp/projects/data/eval/Belle_open_source_0.5M.json

deepspeed_config_file=ds_zero2_no_offload.json

########å¯åŠ¨å‘½ä»¤########
torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \
    --deepspeed ${deepspeed_config_file} \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${chinese_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --do_eval \
    --seed $RANDOM \
    --fp16 \
    --max_steps ${training_steps} \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.03 \
    --weight_decay 0 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --evaluation_strategy steps \
    --eval_steps 250 \
    --save_steps 500 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --max_seq_length 512 \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_dropout ${lora_dropout} \
    --torch_dtype float16 \
    --validation_file ${validation_file} \
    --peft_path ${peft_model} \
    --ddp_find_unused_parameters False
```





##### 7.1.2 æƒ…æ™¯2ï¼šåŸºäºä¸­æ–‡Chinese-LLaMAè®­ç»ƒå…¨æ–°çš„æŒ‡ä»¤ç²¾è°ƒLoRAæƒé‡

**ç¬¬1æ­¥ï¼šåˆå¹¶HFæ ¼å¼çš„LLaMAæ¨¡å‹ä¸chinese_llama_plus_lora_7bæ¨¡å‹**

å‘½ä»¤å¦‚ä¸‹ï¼ˆæ³¨æ„è·¯å¾„ï¼‰ï¼š

```shell
python scripts/merge_llama_with_chinese_lora.py \
--base_model /root/autodl-tmp/llama_7b_hf/ \
--lora_model /root/autodl-tmp/chinese_llama_plus_lora_7b/ \
--output_type huggingface \
--output_dir /root/autodl-tmp/merge_llama_with_llama_lora_hf/
```

**ç¬¬2æ­¥ï¼šåˆå¹¶åæ£€æŸ¥ï¼ˆé‡è¦ï¼ï¼‰**

Chinese-LLaMA-Plus-7B çš„ SHA256 :

`f8d380d63f77a08b7f447f5ec63f0bb1cde9ddeae2207e9f86e6b5f0f95a7955`

å‘½ä»¤å¦‚ä¸‹ï¼š

```shell
sha256sum consolidated.00.pth
```

æˆªå›¾å¦‚ä¸‹ï¼š

![](./images/03.png)

**ç¬¬3æ­¥ï¼šåŸºäºä¸­æ–‡Chinese-LLaMAè®­ç»ƒå…¨æ–°çš„æŒ‡ä»¤ç²¾è°ƒLoRAæƒé‡**

- `--model_name_or_path`: åˆå¹¶å¯¹åº”Chinese-LLaMA-LoRAåçš„HFæ ¼å¼Chinese-LLaMAæ¨¡å‹ï¼ˆæ— è®ºæ˜¯å¦æ˜¯Plusæ¨¡å‹ï¼‰
- `--peft_path`: å‹¿æä¾›æ­¤å‚æ•°ï¼Œå¹¶ä¸”ä»è„šæœ¬ä¸­åˆ é™¤ `--peft_path`
- éœ€æŒ‡å®š`--lora_rank`ã€`--lora_alpha`ã€`--lora_dropout`ã€`--trainable`å’Œ`--modules_to_save`å‚æ•°

```shell
########å‚æ•°éƒ¨åˆ†########
lr=1e-4
lora_rank=8
lora_alpha=32
#lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
lora_trainable="q_proj,v_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=/root/autodl-tmp/merge_llama_with_llama_lora_hf
chinese_tokenizer_path=/root/autodl-tmp/projects/scripts/merged_tokenizer_hf
dataset_dir=/root/autodl-tmp/projects/data/train
per_device_train_batch_size=1
per_device_eval_batch_size=1
training_steps=100
gradient_accumulation_steps=1
output_dir=/root/autodl-tmp/sft_output
validation_file=/root/autodl-tmp/projects/data/eval/Belle_open_source_0.5M.json

deepspeed_config_file=ds_zero2_no_offload.json

########å¯åŠ¨å‘½ä»¤########
torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \
    --deepspeed ${deepspeed_config_file} \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${chinese_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --do_eval \
    --seed $RANDOM \
    --fp16 \
    --max_steps ${training_steps} \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.03 \
    --weight_decay 0 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --evaluation_strategy steps \
    --eval_steps 250 \
    --save_steps 500 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --max_seq_length 512 \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --torch_dtype float16 \
    --validation_file ${validation_file} \
    --gradient_checkpointing \
    --ddp_find_unused_parameters False
```



### ç¬¬8æ­¥ï¼šä¸LangChainè¿›è¡Œé›†æˆ

[å‚è€ƒå®˜æ–¹é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%B8%8ELangChain%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90)

#### 8.1 å¦‚ä½•åœ¨LangChainä¸­ä½¿ç”¨Chinese-Alpacaï¼Ÿ

å› ä¸ºå°†LoRAæƒé‡åˆå¹¶è¿›LLaMAåçš„æ¨¡å‹ä¸åŸç‰ˆLLaMAé™¤äº†è¯è¡¨ä¸åŒä¹‹å¤–ç»“æ„ä¸Šæ²¡æœ‰å…¶ä»–åŒºåˆ«ï¼Œå› æ­¤å¯ä»¥å‚è€ƒä»»ä½•åŸºäºLLaMAçš„LangChainæ•™ç¨‹è¿›è¡Œé›†æˆã€‚ ä»¥ä¸‹æ–‡æ¡£é€šè¿‡ä¸¤ä¸ªç¤ºä¾‹ï¼Œåˆ†åˆ«ä»‹ç»åœ¨LangChainä¸­å¦‚ä½•ä½¿ç”¨Chinese-Alpacaå®ç°

- æ£€ç´¢å¼é—®ç­”
- æ‘˜è¦ç”Ÿæˆ

ä¾‹å­ä¸­çš„è¶…å‚ã€promptæ¨¡ç‰ˆå‡æœªè°ƒä¼˜ï¼Œä»…ä¾›æ¼”ç¤ºå‚è€ƒç”¨ã€‚



##### 8.1.1 å‡†å¤‡å·¥ä½œ

**ç¬¬1æ­¥ï¼šç¯å¢ƒå‡†å¤‡**

```shell
pip install langchain
pip install sentence_transformers faiss-cpu
```

**ä»æºç å®‰è£…[commit idä¸º13e53fcçš„Peft](https://github.com/huggingface/peft/tree/13e53fc)**

```shell
pip install git+https://github.com/huggingface/peft.git@13e53fc
```



**ç¬¬2æ­¥ï¼šæ¨¡å‹åˆå¹¶**

* å¤šLoRAæƒé‡åˆå¹¶ï¼ˆé€‚ç”¨äºChinese-Alpaca-Plus ï¼‰

```shell
python scripts/merge_llama_with_chinese_lora.py \
--base_model /root/autodl-tmp/7B_hf/ \
--lora_model /root/autodl-tmp/chinese_llama_plus_lora_7b/,/root/autodl-tmp/chinese_alpaca_plus_lora_7b \
--output_type huggingface \ 
--output_dir /root/autodl-tmp/7B_merge_model/
```

âš ï¸ **ä¸¤ä¸ªLoRAæ¨¡å‹çš„é¡ºåºå¾ˆé‡è¦ï¼Œä¸èƒ½é¢ å€’ã€‚å…ˆå†™LLaMA-Plus-LoRAç„¶åå†™Alpaca-Plus-LoRAã€‚**



**ç¬¬3æ­¥ï¼šåˆå¹¶åæ£€æŸ¥ï¼ˆé‡è¦ï¼ï¼‰**

<font color=red>**åˆå¹¶å®ŒæˆååŠ¡å¿…æ£€æŸ¥SHA256ï¼è¿™é‡Œæ˜¯é’ˆå¯¹äºpthæ ¼å¼çš„æ¨¡å‹ï¼Œè€Œéhuggingfaceæ ¼å¼çš„æ¨¡å‹**</font>

```shell
sha256sum consolidated.00.pth
```



##### 8.1.2 æ£€ç´¢å¼é—®ç­”

è¯¥ä»»åŠ¡ä½¿ç”¨LLMå®Œæˆé’ˆå¯¹ç‰¹å®šæ–‡æ¡£çš„è‡ªåŠ¨é—®ç­”ï¼Œæµç¨‹åŒ…æ‹¬ï¼šæ–‡æœ¬è¯»å–ã€æ–‡æœ¬åˆ†å‰²ã€æ–‡æœ¬/é—®é¢˜å‘é‡åŒ–ã€æ–‡æœ¬-é—®é¢˜åŒ¹é…ã€å°†åŒ¹é…æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ç»„åˆç”Ÿæˆå¯¹åº”Promptä¸­ä½œä¸ºLLMçš„è¾“å…¥ã€ç”Ÿæˆå›ç­”ã€‚

```shell
python langchain_qa.py \
--embedding_path /root/autodl-tmp/text2vec-large-chinese/ \
--model_path /root/autodl-tmp/7B_merge_model/ \
--file_path /root/autodl-tmp/Chinese-LLaMA-AIpaca-4.0/scripts/langchain/doc.txt \
--chain_type refine
```

å‚æ•°è¯´æ˜ï¼š

- `--embedding_path`: ä¸‹è½½è‡³æœ¬åœ°çš„embedding modelæ‰€åœ¨ç›®å½•ï¼ˆå¦‚`text2vec-large-chinese`ï¼‰æˆ–HuggingFaceæ¨¡å‹åï¼ˆå¦‚`GanymedeNil/text2vec-large-chinese`ï¼‰
- `--model_path`: åˆå¹¶åçš„Alpacaæ¨¡å‹æ‰€åœ¨ç›®å½•
- `--file_path`: å¾…è¿›è¡Œæ£€ç´¢ä¸æé—®çš„æ–‡æ¡£
- `--chain_type`: å¯ä»¥ä¸º`refine`(é»˜è®¤)æˆ–`stuff`ï¼Œä¸ºä¸¤ç§ä¸åŒçš„chainï¼Œè¯¦ç»†è§£é‡Šè§[è¿™é‡Œ](https://docs.langchain.com/docs/components/chains/index_related_chains)ã€‚ç®€å•æ¥è¯´ï¼Œstuffé€‚ç”¨äºè¾ƒçŸ­çš„ç¯‡ç« ï¼Œè€Œrefineé€‚ç”¨äºè¾ƒé•¿çš„ç¯‡ç« ã€‚
- `--gpus {gpu_ids}`: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚`0,1,2`

âš ï¸ **ä¿®æ”¹LangChainä¸­`langchain.HuggingFacePipeline.from_model_id`ä¸­çš„ç›¸å…³ä»£ç ï¼Œå°†å…¶ä¸­tokenizerçš„åˆå§‹åŒ–éƒ¨åˆ†**

```shell
ä»£ç è·¯å¾„ï¼š/root/miniconda3/lib/python3.8/site-packages/langchain/llms
```

```shell
tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)
```

ä¿®æ”¹ä¸ºï¼š

```shell
tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, **_model_kwargs)
```



##### 8.1.2 æ‘˜è¦ç”Ÿæˆ

è¯¥ä»»åŠ¡ä½¿ç”¨LLMå®Œæˆç»™å®šæ–‡æ¡£çš„æ‘˜è¦ç”Ÿæˆï¼Œä»¥å¸®åŠ©æç‚¼æ–‡æ¡£ä¸­çš„æ ¸å¿ƒä¿¡æ¯ã€‚

```shell
python langchain_sum.py \
--model_path /root/autodl-tmp/7B_merge_model/ \
--file_path /root/autodl-tmp/Chinese-LLaMA-AIpaca-4.0/scripts/langchain/sample.txt \
--chain_type refine
```

å‚æ•°è¯´æ˜ï¼š

- `--model_path`: åˆå¹¶åçš„Alpacaæ¨¡å‹æ‰€åœ¨ç›®å½•
- `--file_path`: å¾…è¿›è¡Œæ‘˜è¦çš„æ–‡æ¡£
- `--chain_type`: å¯ä»¥ä¸º`refine`(é»˜è®¤)æˆ–`stuff`ï¼Œä¸ºä¸¤ç§ä¸åŒçš„chainï¼Œè¯¦ç»†è§£é‡Šè§[è¿™é‡Œ](https://docs.langchain.com/docs/components/chains/index_related_chains)ã€‚ç®€å•æ¥è¯´ï¼Œstuffé€‚ç”¨äºè¾ƒçŸ­çš„ç¯‡ç« ï¼Œè€Œrefineé€‚ç”¨äºè¾ƒé•¿çš„ç¯‡ç« ã€‚
- `--gpus {gpu_ids}`: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚`0,1,2`





### ç¬¬9æ­¥ï¼šä½¿ç”¨FastAPIæ·»åŠ OpenAI API ç¤ºä¾‹

#### 9.1 ç¯å¢ƒé…ç½®

æ–°å¢å®‰è£…ç›¸å…³çš„ä¾èµ–åŒ…ï¼š

```shell
pip install requests fastapi uvicorn shortuuid
```



#### 9.2 å¯åŠ¨æœåŠ¡

```shell
python openai_api_server.py --base_model /root/autodl-tmp/7B_merge_model/ --tokenizer_path /root/autodl-tmp/7B_merge_model/tokenizer.model
```



#### 9.3 æµ‹è¯• APIs

â‘  æµ‹è¯• completions

```shell
curl http://localhost:19327/v1/completions \
  -H "Content-Type: application/json" \
  -d '{   
    "prompt": "å‘Šè¯‰æˆ‘ä¸­å›½çš„é¦–éƒ½åœ¨å“ªé‡Œ"
  }'
```



â‘¡ æµ‹è¯• chat completions

```shell
curl http://localhost:19327/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{   
    "messages": [
      {"role": "user","message": "ç»™æˆ‘è®²ä¸€äº›æœ‰å…³æ­å·çš„æ•…äº‹å§"}
    ],
    "repetition_penalty": 1.0
  }'
```



â‘¢ æµ‹è¯• embeddings

```shell
curl http://localhost:19327/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "ä»Šå¤©å¤©æ°”çœŸä¸é”™"
  }'
```













## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæºç è®²è§£ï¼ˆæ ¸å¿ƒä»£ç é€ä¸€è®²è§£ï¼‰



### ä»£ç -1 crawl_prompt.py &#x2705;



### ä»£ç -2 merge_tokenizers.py &#x2705;



### ä»£ç -3 merge_llama_with_lora.py &#x2705;



### ä»£ç -4 inference_hf.py &#x2705;



